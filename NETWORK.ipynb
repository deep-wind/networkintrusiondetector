{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d3e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "#from featureencodinglibrary import featureEncodingUsingLabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#feature scaling library\n",
    "#from featurescalinglibrary import featureScalingUsingStandardScalar\n",
    "\n",
    "\n",
    "#Libraries for printing tables in readable format\n",
    "from tabulate import tabulate\n",
    "\n",
    "#Library for creating an excel sheet\n",
    "import xlsxwriter\n",
    "\n",
    "labelName = 'attack_type'\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf571f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This function is used to perform label encoding on the categorical features in the given dataset\n",
    "def featureEncodingUsingLabelEncoder(dataSetForFeatureEncoding):\n",
    "    print(\"****** Start label encoding on the categorical features in the given dataset *****\")\n",
    "\n",
    "    #Extract the categorical features, leave the label\n",
    "    categoricalColumnsInTheDataSet = dataSetForFeatureEncoding.drop([labelName],axis=1).select_dtypes(['object'])\n",
    "    #Get the names of the categorical features\n",
    "    categoricalColumnNames = categoricalColumnsInTheDataSet.columns.values\n",
    " \n",
    "    print(\"****** Number of features before label encoding: \",len(dataSetForFeatureEncoding.columns))\n",
    "    print(\"****** Number of categorical features in the dataset: \",len(categoricalColumnNames))\n",
    "    print(\"****** Categorical feature names in the dataset: \",categoricalColumnNames)\n",
    "\n",
    "    print('\\n****** Here is the list of unique values present in each categorical feature in the dataset *****\\n')\n",
    "    labelEncoder = LabelEncoder() \n",
    "    for feature in categoricalColumnNames:\n",
    "        uniq = np.unique(dataSetForFeatureEncoding[feature])\n",
    "        print('\\n{}: {} '.format(feature,len(uniq)))\n",
    "        printList(dataSetForFeatureEncoding[feature].unique(),'distinct values')\n",
    "        dataSetForFeatureEncoding[feature] = labelEncoder.fit_transform(dataSetForFeatureEncoding[feature]) \n",
    "    print(\"****** Number of features after label encoding: \",len(dataSetForFeatureEncoding.columns))    \n",
    "    \n",
    "    print(\"****** End label encoding on the categorical features in the given dataset *****\\n\")\n",
    "    return dataSetForFeatureEncoding\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b32a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2823cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createExcelFromArray(array, fileName):\n",
    "    workbook = xlsxwriter.Workbook(fileName)\n",
    "    worksheet = workbook.add_worksheet()\n",
    "\n",
    "    row = 0\n",
    "    for col, data in enumerate(array):\n",
    "        worksheet.write_row(col, row, data)\n",
    "\n",
    "    workbook.close()\n",
    "\n",
    "def printList (list,heading):\n",
    "    for i in range(0, len(list)): \n",
    "        list[i] = str(list[i]) \n",
    "    if len(list)>0:\n",
    "        print(tabulate([i.strip(\"[]\").split(\", \") for i in list], headers=[heading], tablefmt='orgtbl')+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebc251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def featureSelectionUsingExtraTreesClassifier(dataSetForFeatureSelection):\n",
    "    print(\"\\n****** Start performing feature selection using ExtraTreesClassifier *****\")\n",
    "    print(\"****** Falls under wrapper methods (feature importance) *****\")\n",
    "\n",
    "\n",
    "    #Applying feature encoding before applying the ExtraTreesClassification\n",
    "    dataSetForFeatureSelection = featureEncodingUsingLabelEncoder(dataSetForFeatureSelection)\n",
    "    dataSetAfterFeatuerSelection = dataSetForFeatureSelection\n",
    "    #features = dataSetForFeatureSelection.iloc[:,0:len(dataSetForFeatureSelection.columns)-1]  \n",
    "    features = dataSetForFeatureSelection.drop([labelName],axis=1)\n",
    "    label = dataSetForFeatureSelection[labelName]\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "    labelTransformed = labelencoder.fit_transform(label)\n",
    "\t\n",
    "    print(\"****** ExtraTreesClassification is in progress *****\")\n",
    "    #Train using ExtraTreesClassifier\n",
    "    trainedforest = ExtraTreesClassifier(n_estimators=700).fit(features,labelTransformed)\n",
    "    importances = trainedforest.feature_importances_ #array with importances of each feature\n",
    "    idx = np.arange(0, features.shape[1]) #create an index array, with the number of features\n",
    "    features_to_keep = idx[importances > np.mean(importances)] #only keep features whose importance is greater than the mean importance\n",
    "    featureImportances = pd.Series(importances, index= features.columns)\n",
    "    selectedFeatures = featureImportances.nlargest(len(features_to_keep))\n",
    "    print(\"\\n selectedFeatures after ExtraTreesClassification: \", selectedFeatures)\n",
    "    print(\"****** Completed ExtraTreesClassification *****\")\n",
    "\n",
    "    #Plot the feature Importance to see which features have been considered as most important for our model to make its predictions\n",
    "    #figure(num=None, figsize=(20, 22), dpi=80, facecolor='w', edgecolor='k')\n",
    "    #selectedFeatures.plot(kind='barh')\n",
    "\n",
    "    selectedFeaturesNames = selectedFeatures.keys()\n",
    "    dataSetForFeatureSelection = dataSetForFeatureSelection.drop(selectedFeaturesNames,axis=1)\n",
    "    dataSetAfterFeatuerSelection = dataSetAfterFeatuerSelection.drop(dataSetForFeatureSelection.columns, axis=1)\n",
    "    dataSetAfterFeatuerSelection[labelName] = label\n",
    "    \n",
    "    numberOfFeaturesInTheDatasetAfterFeatureSelection = len(dataSetAfterFeatuerSelection.columns)    \n",
    "    print('\\n***** Number of columns in the dataSet after feature selection: ', len(dataSetAfterFeatuerSelection.columns))\n",
    "    print('***** Columns in the dataSet after feature selection: \\n', dataSetAfterFeatuerSelection.columns)\n",
    "    print(\"****** End performing feature selection using ExtraTreesClassifier *****\")\n",
    "    return dataSetAfterFeatuerSelection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba42fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEncodingUsingOneHotEncoder(dataSetForFeatureEncoding):\n",
    "    print(\"****** Start one hot encoding on the categorical features in the given dataset *****\")\n",
    "    #Extract the categorical features, leave the label\n",
    "    categoricalColumnsInTheDataSet = dataSetForFeatureEncoding.drop([labelName],axis=1).select_dtypes(['object'])\n",
    "    #Get the names of the categorical features\n",
    "    categoricalColumnNames = categoricalColumnsInTheDataSet.columns.values\n",
    "    \n",
    "    print(\"****** Number of features before one hot encoding: \",len(dataSetForFeatureEncoding.columns))\n",
    "    print(\"****** Number of categorical features in the dataset: \",len(categoricalColumnNames))\n",
    "    print(\"****** Categorical feature names in the dataset: \",categoricalColumnNames)\n",
    "    \n",
    "    print('\\n****** Here is the list of unique values present in each categorical feature in the dataset *****\\n')\n",
    "    categoricalFeaturesInTheDataset = list(set(dataSetForFeatureEncoding.columns) - set(dataSetForFeatureEncoding._get_numeric_data().columns))\n",
    "    numericalFeaturesInTheDataset = list(dataSetForFeatureEncoding._get_numeric_data().columns)\n",
    "    for feature in categoricalFeaturesInTheDataset:\n",
    "        uniq = np.unique(dataSetForFeatureEncoding[feature])\n",
    "        print('\\n{}: {} '.format(feature,len(uniq)))\n",
    "        printList(dataSetForFeatureEncoding[feature].unique(),'distinct values')\n",
    "        \n",
    "    #Using get_dummies function to get the dummy variables for the categorical columns\n",
    "    onHotEncodedDataSet=pd.get_dummies(dataSetForFeatureEncoding, columns=categoricalColumnNames, prefix=categoricalColumnNames)\n",
    "    \n",
    "    #Move the label column to the end\n",
    "    label = onHotEncodedDataSet.pop(labelName)\n",
    "    onHotEncodedDataSet[labelName] = label\n",
    "    numberOfColumnsInOneHotEncodedDataset = len(onHotEncodedDataSet.columns)\n",
    "    print(\"****** Number of features after one hot encoding: \",numberOfColumnsInOneHotEncodedDataset)\n",
    "\n",
    "    print(\"****** End one hot encoding on the categorical features in the given dataset *****\\n\")\n",
    "    return onHotEncodedDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946eae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "def featureScalingUsingStandardScalar(dataSetForFeatureScaling):\n",
    "    print(\"****** Start feature scaling of the features present in the dataset using StandardScalar *****\")\n",
    "\n",
    "    numberOfColumnsInEncodedDataset = len(dataSetForFeatureScaling.columns)\n",
    "    dataSetInArrayFormat = dataSetForFeatureScaling.values\n",
    "\n",
    "    #Remove the label column from the dataset\n",
    "    \n",
    "    label = dataSetForFeatureScaling.pop(labelName)\n",
    "\n",
    "    print(dataSetInArrayFormat)\n",
    "    features = dataSetInArrayFormat[:,0:numberOfColumnsInEncodedDataset-1]\n",
    "    print(\"\\n****** Number of features in the dataset before performing scaling: \",np.size(features,1))\n",
    "    print(\"\\n****** Features in the dataset before performing scaling ***** \\n\",features)\n",
    "    \n",
    "    #Perform feature scaling\n",
    "    scaler=StandardScaler()\n",
    "    scaledFeatures=scaler.fit_transform(features)    \n",
    "    print(\"\\n****** Number of features in the dataset after performing scaling: \",np.size(scaledFeatures,1))\n",
    "    print(\"\\n****** Features in the dataset after performing scaling ***** \\n\",scaledFeatures)\n",
    "\n",
    "    #Convert from array format to dataframe\n",
    "    scaledFeatures = pd.DataFrame(scaledFeatures, columns=dataSetForFeatureScaling.columns)\n",
    "    scaledFeatures = scaledFeatures.reset_index(drop=True)\n",
    "    label = label.reset_index(drop=True)\n",
    "    scaledFeatures[labelName]=label\n",
    "    print(\"scaledFeatures.head(): \",scaledFeatures.head())\n",
    "    print(\"scaledFeatures.shape: \",scaledFeatures.shape)\n",
    "    \n",
    "    print(\"\\n****** End of feature scaling of the features present in the dataset using StandardScalar *****\\n\")\n",
    "    return scaledFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7a888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the complete dataSet into training dataSet and testing dataSet\n",
    "def splitCompleteDataSetIntoTrainingSetAndTestingSet(completeDataSet):\n",
    "\tlabel = completeDataSet[labelName]\n",
    "\tfeatures = completeDataSet.drop(labelName,axis=1)\n",
    "\tfeaturesInPreProcessedTrainingDataSet,featuresInPreProcessedTestingDataSet,labelInPreProcessedTrainingDataSet,labelInPreProcessedTestingDataSet=train_test_split(features,label,test_size=0.4, random_state=42)\n",
    "\tprint(\"features.shape: \",features.shape)\n",
    "\tprint(\"label.shape: \",label.shape)\n",
    "\tprint(\"features: \",features)\n",
    "\tprint(\"label: \",label)\n",
    "\treturn featuresInPreProcessedTrainingDataSet,featuresInPreProcessedTestingDataSet,labelInPreProcessedTrainingDataSet,labelInPreProcessedTestingDataSet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd34a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performPreprocessing(trainingDataSet, testingDataSet, arrayOfModels):\n",
    "    for i in range(0,len(arrayOfModels)):\n",
    "        print('***************************************************************************************************************************')\n",
    "        print('********************************************* Building Model-', i ,' As Below *************************************************')\n",
    "        print('\\t -- Feature Selection: \\t ', arrayOfModels[i][0], ' \\n\\t -- Feature Encoding: \\t ', arrayOfModels[i][1], ' \\n\\t -- Feature Scaling: \\t ', arrayOfModels[i][2], '\\n')\n",
    "\n",
    "        #Combining the test and training datasets for preprocessing then together, because we observed that in sme datasets\n",
    "        #the values in the categorical columns in test dataset and train dataset are being different this causes issues while\n",
    "        #applying classification techniques\n",
    "        completeDataSet = pd.concat(( trainingDataSet, testingDataSet ))\n",
    "\n",
    "        #difficultyLevel = completeDataSet.pop('difficulty_level')\n",
    "        \n",
    "        print(\"completeDataSet.shape: \",completeDataSet.shape)\n",
    "        print(\"completeDataSet.head: \",completeDataSet.head())\n",
    "\n",
    "        #Feature Selection  \n",
    "        if arrayOfModels[i][0] == 'ExtraTreesClassifier':\n",
    "            #Perform feature selection using ExtraTreesClassifier\n",
    "            completeDataSetAfterFeatuerSelection = featureSelectionUsingExtraTreesClassifier(completeDataSet)        \n",
    "        #Feature Encoding        \n",
    " \n",
    "        if arrayOfModels[i][1] == 'OneHotEncoder':\n",
    "            #Perform OnHot encoding to convert categorical values into one-hot encoded features\n",
    "            completeEncodedDataSet = featureEncodingUsingOneHotEncoder(completeDataSetAfterFeatuerSelection)\n",
    "\n",
    "        #Feature Scaling        \n",
    " \n",
    "        if arrayOfModels[i][2] == 'Standardization':\n",
    "            #Perform Standardization to scale the features of the dataset into same range\n",
    "            completeEncodedAndScaledDataset = featureScalingUsingStandardScalar(completeEncodedDataSet)\n",
    "        \n",
    "        #Split the complete dataSet into training dataSet and testing dataSet\n",
    "        featuresInPreProcessedTrainingDataSet,featuresInPreProcessedTestingDataSet,labelInPreProcessedTrainingDataSet,labelInPreProcessedTestingDataSet = splitCompleteDataSetIntoTrainingSetAndTestingSet(completeEncodedAndScaledDataset)\n",
    "        \n",
    "        trainingEncodedAndScaledDataset = pd.concat([featuresInPreProcessedTrainingDataSet, labelInPreProcessedTrainingDataSet], axis=1, sort=False)\n",
    "        testingEncodedAndScaledDataset = pd.concat([featuresInPreProcessedTestingDataSet, labelInPreProcessedTestingDataSet], axis=1, sort=False)\n",
    "    \n",
    "    return \tcompleteEncodedAndScaledDataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04eb8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Duration Protocol_type  Service Flag  Src_bytes  Dst_bytes  Land  \\\n",
      "0         0           udp    other   SF        146          0     0   \n",
      "1         0           tcp  private   S0          0          0     0   \n",
      "2         0           tcp     http   SF        232       8153     0   \n",
      "3         0           tcp     http   SF        199        420     0   \n",
      "4         0           tcp  private  REJ          0          0     0   \n",
      "\n",
      "   Wrong_fragment  Urgent  Hot  ...  Dst_host_srv_count  \\\n",
      "0               0       0    0  ...                   1   \n",
      "1               0       0    0  ...                  26   \n",
      "2               0       0    0  ...                 255   \n",
      "3               0       0    0  ...                 255   \n",
      "4               0       0    0  ...                  19   \n",
      "\n",
      "   Dst_host_same_srv_rate  Dst_host_diff_srv_rate  \\\n",
      "0                    0.00                    0.60   \n",
      "1                    0.10                    0.05   \n",
      "2                    1.00                    0.00   \n",
      "3                    1.00                    0.00   \n",
      "4                    0.07                    0.07   \n",
      "\n",
      "   Dst_host_same_src_port_rate  Dst_host_srv_diff_host_rate  \\\n",
      "0                         0.88                         0.00   \n",
      "1                         0.00                         0.00   \n",
      "2                         0.03                         0.04   \n",
      "3                         0.00                         0.00   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   Dst_host_serror_rate  Dst_host_srv_serror_rate  Dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                   0.0   \n",
      "1                  1.00                      1.00                   0.0   \n",
      "2                  0.03                      0.01                   0.0   \n",
      "3                  0.00                      0.00                   0.0   \n",
      "4                  0.00                      0.00                   1.0   \n",
      "\n",
      "   Dst_host_srv_rerror_rate  attack_type  \n",
      "0                      0.00       normal  \n",
      "1                      0.00      neptune  \n",
      "2                      0.01       normal  \n",
      "3                      0.00       normal  \n",
      "4                      1.00      neptune  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "   Duration Protocol_type  Service Flag  Src_bytes  Dst_bytes  Land  \\\n",
      "0        13           tcp   telnet   SF        118       2425     0   \n",
      "1         0           udp  private   SF         44          0     0   \n",
      "2         0           tcp   telnet   S3          0         44     0   \n",
      "3         0           udp  private   SF         53         55     0   \n",
      "4         0           tcp  private   SH          0          0     0   \n",
      "\n",
      "   Wrong_fragment  Urgent  Hot  ...  Dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  10   \n",
      "1               0       0    0  ...                 254   \n",
      "2               0       0    0  ...                  79   \n",
      "3               0       0    0  ...                 255   \n",
      "4               0       0    0  ...                   1   \n",
      "\n",
      "   Dst_host_same_srv_rate  Dst_host_diff_srv_rate  \\\n",
      "0                    0.38                    0.12   \n",
      "1                    1.00                    0.01   \n",
      "2                    0.31                    0.61   \n",
      "3                    1.00                    0.00   \n",
      "4                    0.06                    1.00   \n",
      "\n",
      "   Dst_host_same_src_port_rate  Dst_host_srv_diff_host_rate  \\\n",
      "0                         0.04                          0.0   \n",
      "1                         0.01                          0.0   \n",
      "2                         0.00                          0.0   \n",
      "3                         0.87                          0.0   \n",
      "4                         1.00                          0.0   \n",
      "\n",
      "   Dst_host_serror_rate  Dst_host_srv_serror_rate  Dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                  0.12   \n",
      "1                  0.00                      0.00                  0.00   \n",
      "2                  0.21                      0.68                  0.60   \n",
      "3                  0.00                      0.00                  0.00   \n",
      "4                  1.00                      1.00                  0.00   \n",
      "\n",
      "   Dst_host_srv_rerror_rate   attack_type  \n",
      "0                       0.3  guess_passwd  \n",
      "1                       0.0     snmpguess  \n",
      "2                       0.0  processtable  \n",
      "3                       0.0        normal  \n",
      "4                       0.0          nmap  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arrayOfModels = [[\"ExtraTreesClassifier\",\"OneHotEncoder\",\"Standardization\"]]\n",
    "\n",
    "\n",
    "trainingFileNameWithAbsolutePath = r\"C:\\Users\\PRAMILA\\Downloads\\NetworkIntrusionDetection-master\\NetworkIntrusionDetection-master\\Datasets\\NSL-KDD\\KDDTrain+_20Percent.csv\"\n",
    "testingFileNameWithAbsolutePath = r\"C:\\Users\\PRAMILA\\Downloads\\NetworkIntrusionDetection-master\\NetworkIntrusionDetection-master\\Datasets\\NSL-KDD\\KDDTest-21.csv\"\n",
    "\n",
    "#Train data\n",
    "trainingDataSet =  pd.read_csv(trainingFileNameWithAbsolutePath)\n",
    "difficultyLevel = trainingDataSet.pop('difficulty_level')\n",
    "label = trainingDataSet[labelName]\n",
    "print(trainingDataSet.head())\n",
    "\n",
    "#Test data\n",
    "testingDataSet = pd.read_csv(testingFileNameWithAbsolutePath)\n",
    "difficultyLevel = testingDataSet.pop('difficulty_level')\n",
    "print(testingDataSet.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30f0d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************************************\n",
      "********************************************* Building Model- 0  As Below *************************************************\n",
      "\t -- Feature Selection: \t  ExtraTreesClassifier  \n",
      "\t -- Feature Encoding: \t  OneHotEncoder  \n",
      "\t -- Feature Scaling: \t  Standardization \n",
      "\n",
      "completeDataSet.shape:  (37041, 42)\n",
      "completeDataSet.head:     Duration Protocol_type  Service Flag  Src_bytes  Dst_bytes  Land  \\\n",
      "0         0           udp    other   SF        146          0     0   \n",
      "1         0           tcp  private   S0          0          0     0   \n",
      "2         0           tcp     http   SF        232       8153     0   \n",
      "3         0           tcp     http   SF        199        420     0   \n",
      "4         0           tcp  private  REJ          0          0     0   \n",
      "\n",
      "   Wrong_fragment  Urgent  Hot  ...  Dst_host_srv_count  \\\n",
      "0               0       0    0  ...                   1   \n",
      "1               0       0    0  ...                  26   \n",
      "2               0       0    0  ...                 255   \n",
      "3               0       0    0  ...                 255   \n",
      "4               0       0    0  ...                  19   \n",
      "\n",
      "   Dst_host_same_srv_rate  Dst_host_diff_srv_rate  \\\n",
      "0                    0.00                    0.60   \n",
      "1                    0.10                    0.05   \n",
      "2                    1.00                    0.00   \n",
      "3                    1.00                    0.00   \n",
      "4                    0.07                    0.07   \n",
      "\n",
      "   Dst_host_same_src_port_rate  Dst_host_srv_diff_host_rate  \\\n",
      "0                         0.88                         0.00   \n",
      "1                         0.00                         0.00   \n",
      "2                         0.03                         0.04   \n",
      "3                         0.00                         0.00   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   Dst_host_serror_rate  Dst_host_srv_serror_rate  Dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                   0.0   \n",
      "1                  1.00                      1.00                   0.0   \n",
      "2                  0.03                      0.01                   0.0   \n",
      "3                  0.00                      0.00                   0.0   \n",
      "4                  0.00                      0.00                   1.0   \n",
      "\n",
      "   Dst_host_srv_rerror_rate  attack_type  \n",
      "0                      0.00       normal  \n",
      "1                      0.00      neptune  \n",
      "2                      0.01       normal  \n",
      "3                      0.00       normal  \n",
      "4                      1.00      neptune  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "\n",
      "****** Start performing feature selection using ExtraTreesClassifier *****\n",
      "****** Falls under wrapper methods (feature importance) *****\n",
      "****** Start label encoding on the categorical features in the given dataset *****\n",
      "****** Number of features before label encoding:  42\n",
      "****** Number of categorical features in the dataset:  3\n",
      "****** Categorical feature names in the dataset:  ['Protocol_type' 'Service' 'Flag']\n",
      "\n",
      "****** Here is the list of unique values present in each categorical feature in the dataset *****\n",
      "\n",
      "\n",
      "Protocol_type: 3 \n",
      "| distinct values   |\n",
      "|-------------------|\n",
      "| udp               |\n",
      "| tcp               |\n",
      "| icmp              |\n",
      "\n",
      "\n",
      "Service: 67 \n",
      "| distinct values   |\n",
      "|-------------------|\n",
      "| other             |\n",
      "| private           |\n",
      "| http              |\n",
      "| remote_job        |\n",
      "| ftp_data          |\n",
      "| name              |\n",
      "| netbios_ns        |\n",
      "| eco_i             |\n",
      "| mtp               |\n",
      "| telnet            |\n",
      "| finger            |\n",
      "| domain_u          |\n",
      "| supdup            |\n",
      "| uucp_path         |\n",
      "| Z39_50            |\n",
      "| smtp              |\n",
      "| csnet_ns          |\n",
      "| uucp              |\n",
      "| netbios_dgm       |\n",
      "| urp_i             |\n",
      "| auth              |\n",
      "| domain            |\n",
      "| ftp               |\n",
      "| bgp               |\n",
      "| ldap              |\n",
      "| ecr_i             |\n",
      "| gopher            |\n",
      "| vmnet             |\n",
      "| systat            |\n",
      "| http_443          |\n",
      "| efs               |\n",
      "| whois             |\n",
      "| imap4             |\n",
      "| iso_tsap          |\n",
      "| echo              |\n",
      "| klogin            |\n",
      "| link              |\n",
      "| sunrpc            |\n",
      "| login             |\n",
      "| kshell            |\n",
      "| sql_net           |\n",
      "| time              |\n",
      "| hostnames         |\n",
      "| exec              |\n",
      "| ntp_u             |\n",
      "| discard           |\n",
      "| nntp              |\n",
      "| courier           |\n",
      "| ctf               |\n",
      "| ssh               |\n",
      "| daytime           |\n",
      "| shell             |\n",
      "| netstat           |\n",
      "| pop_3             |\n",
      "| nnsp              |\n",
      "| IRC               |\n",
      "| pop_2             |\n",
      "| printer           |\n",
      "| tim_i             |\n",
      "| pm_dump           |\n",
      "| red_i             |\n",
      "| netbios_ssn       |\n",
      "| rje               |\n",
      "| X11               |\n",
      "| urh_i             |\n",
      "| http_8001         |\n",
      "| tftp_u            |\n",
      "\n",
      "\n",
      "Flag: 11 \n",
      "| distinct values   |\n",
      "|-------------------|\n",
      "| SF                |\n",
      "| S0                |\n",
      "| REJ               |\n",
      "| RSTR              |\n",
      "| SH                |\n",
      "| RSTO              |\n",
      "| S1                |\n",
      "| RSTOS0            |\n",
      "| S3                |\n",
      "| S2                |\n",
      "| OTH               |\n",
      "\n",
      "****** Number of features after label encoding:  42\n",
      "****** End label encoding on the categorical features in the given dataset *****\n",
      "\n",
      "****** ExtraTreesClassification is in progress *****\n",
      "\n",
      " selectedFeatures after ExtraTreesClassification:  Same_srv_rate                  0.072304\n",
      "Service                        0.059510\n",
      "Dst_host_srv_serror_rate       0.057202\n",
      "Dst_host_same_srv_rate         0.047515\n",
      "Dst_host_serror_rate           0.047391\n",
      "Flag                           0.046447\n",
      "Dst_host_srv_count             0.044726\n",
      "Serror_rate                    0.040804\n",
      "Protocol_type                  0.040505\n",
      "Dst_host_same_src_port_rate    0.039684\n",
      "Srv_serror_rate                0.037258\n",
      "Logged_in                      0.037116\n",
      "Dst_host_rerror_rate           0.036713\n",
      "Count                          0.036695\n",
      "Src_bytes                      0.035562\n",
      "Dst_host_diff_srv_rate         0.031812\n",
      "Dst_host_count                 0.030777\n",
      "Diff_srv_rate                  0.030101\n",
      "Srv_count                      0.024757\n",
      "Dst_host_srv_diff_host_rate    0.024641\n",
      "dtype: float64\n",
      "****** Completed ExtraTreesClassification *****\n",
      "\n",
      "***** Number of columns in the dataSet after feature selection:  21\n",
      "***** Columns in the dataSet after feature selection: \n",
      " Index(['Protocol_type', 'Service', 'Flag', 'Src_bytes', 'Logged_in', 'Count',\n",
      "       'Srv_count', 'Serror_rate', 'Srv_serror_rate', 'Same_srv_rate',\n",
      "       'Diff_srv_rate', 'Dst_host_count', 'Dst_host_srv_count',\n",
      "       'Dst_host_same_srv_rate', 'Dst_host_diff_srv_rate',\n",
      "       'Dst_host_same_src_port_rate', 'Dst_host_srv_diff_host_rate',\n",
      "       'Dst_host_serror_rate', 'Dst_host_srv_serror_rate',\n",
      "       'Dst_host_rerror_rate', 'attack_type'],\n",
      "      dtype='object')\n",
      "****** End performing feature selection using ExtraTreesClassifier *****\n",
      "****** Start one hot encoding on the categorical features in the given dataset *****\n",
      "****** Number of features before one hot encoding:  21\n",
      "****** Number of categorical features in the dataset:  0\n",
      "****** Categorical feature names in the dataset:  []\n",
      "\n",
      "****** Here is the list of unique values present in each categorical feature in the dataset *****\n",
      "\n",
      "\n",
      "attack_type: 40 \n",
      "| distinct values   |\n",
      "|-------------------|\n",
      "| normal            |\n",
      "| neptune           |\n",
      "| warezclient       |\n",
      "| ipsweep           |\n",
      "| portsweep         |\n",
      "| teardrop          |\n",
      "| nmap              |\n",
      "| satan             |\n",
      "| smurf             |\n",
      "| pod               |\n",
      "| back              |\n",
      "| guess_passwd      |\n",
      "| ftp_write         |\n",
      "| multihop          |\n",
      "| rootkit           |\n",
      "| buffer_overflow   |\n",
      "| imap              |\n",
      "| warezmaster       |\n",
      "| phf               |\n",
      "| land              |\n",
      "| loadmodule        |\n",
      "| spy               |\n",
      "| snmpguess         |\n",
      "| processtable      |\n",
      "| saint             |\n",
      "| mscan             |\n",
      "| apache2           |\n",
      "| httptunnel        |\n",
      "| mailbomb          |\n",
      "| snmpgetattack     |\n",
      "| worm              |\n",
      "| sendmail          |\n",
      "| xlock             |\n",
      "| xterm             |\n",
      "| xsnoop            |\n",
      "| ps                |\n",
      "| named             |\n",
      "| udpstorm          |\n",
      "| perl              |\n",
      "| sqlattack         |\n",
      "\n",
      "****** Number of features after one hot encoding:  21\n",
      "****** End one hot encoding on the categorical features in the given dataset *****\n",
      "\n",
      "****** Start feature scaling of the features present in the dataset using StandardScalar *****\n",
      "[[2 41 9 ... 0.0 0.0 'normal']\n",
      " [1 46 5 ... 1.0 0.0 'neptune']\n",
      " [1 22 9 ... 0.01 0.0 'normal']\n",
      " ...\n",
      " [1 57 2 ... 0.08 0.85 'mscan']\n",
      " [1 54 1 ... 0.0 0.88 'mscan']\n",
      " [2 46 9 ... 0.0 0.0 'snmpguess']]\n",
      "\n",
      "****** Number of features in the dataset before performing scaling:  20\n",
      "\n",
      "****** Features in the dataset before performing scaling ***** \n",
      " [[2 41 9 ... 0.0 0.0 0.0]\n",
      " [1 46 5 ... 1.0 1.0 0.0]\n",
      " [1 22 9 ... 0.03 0.01 0.0]\n",
      " ...\n",
      " [1 57 2 ... 0.05 0.08 0.85]\n",
      " [1 54 1 ... 0.0 0.0 0.88]\n",
      " [2 46 9 ... 0.0 0.0 0.0]]\n",
      "\n",
      "****** Number of features in the dataset after performing scaling:  20\n",
      "\n",
      "****** Features in the dataset after performing scaling ***** \n",
      " [[ 2.03857058  0.6299765   0.73536923 ... -0.56138989 -0.54981386\n",
      "  -0.48776502]\n",
      " [-0.15478617  0.93890397 -0.66099165 ...  1.90403374  1.89967409\n",
      "  -0.48776502]\n",
      " [-0.15478617 -0.54394786  0.73536923 ... -0.48742718 -0.52531898\n",
      "  -0.48776502]\n",
      " ...\n",
      " [-0.15478617  1.61854439 -1.70826232 ... -0.43811871 -0.35385482\n",
      "   2.03171007]\n",
      " [-0.15478617  1.43318791 -2.05735254 ... -0.56138989 -0.54981386\n",
      "   2.12063272]\n",
      " [ 2.03857058  0.93890397  0.73536923 ... -0.56138989 -0.54981386\n",
      "  -0.48776502]]\n",
      "scaledFeatures.head():     Protocol_type   Service      Flag  Src_bytes  Logged_in     Count  \\\n",
      "0       2.038571  0.629977  0.735369  -0.011190  -0.732914 -0.581217   \n",
      "1      -0.154786  0.938904 -0.660992  -0.011262  -0.732914  0.275339   \n",
      "2      -0.154786 -0.543948  0.735369  -0.011147   1.364417 -0.643512   \n",
      "3      -0.154786 -0.543948  0.735369  -0.011163   1.364417 -0.448840   \n",
      "4      -0.154786  0.938904 -2.057353  -0.011262  -0.732914  0.259766   \n",
      "\n",
      "   Srv_count  Serror_rate  Srv_serror_rate  Same_srv_rate  ...  \\\n",
      "0  -0.367299    -0.556584        -0.552030      -1.421427  ...   \n",
      "1  -0.312117     1.851192         1.851769      -1.491319  ...   \n",
      "2  -0.323153    -0.075029        -0.071270       0.721924  ...   \n",
      "3  -0.025167    -0.556584        -0.552030       0.721924  ...   \n",
      "4  -0.168642    -0.556584        -0.552030      -1.235049  ...   \n",
      "\n",
      "   Dst_host_count  Dst_host_srv_count  Dst_host_same_srv_rate  \\\n",
      "0        0.656445           -1.050270               -1.193023   \n",
      "1        0.656445           -0.821669               -0.966271   \n",
      "2       -1.709884            1.272317                1.074493   \n",
      "3        0.656445            1.272317                1.074493   \n",
      "4        0.656445           -0.885678               -1.034297   \n",
      "\n",
      "   Dst_host_diff_srv_rate  Dst_host_same_src_port_rate  \\\n",
      "0                2.187298                     2.137976   \n",
      "1               -0.237144                    -0.498320   \n",
      "2               -0.457548                    -0.408446   \n",
      "3               -0.457548                    -0.498320   \n",
      "4               -0.148983                    -0.498320   \n",
      "\n",
      "   Dst_host_srv_diff_host_rate  Dst_host_serror_rate  \\\n",
      "0                    -0.263701             -0.561390   \n",
      "1                    -0.263701              1.904034   \n",
      "2                     0.094049             -0.487427   \n",
      "3                    -0.263701             -0.561390   \n",
      "4                    -0.263701             -0.561390   \n",
      "\n",
      "   Dst_host_srv_serror_rate  Dst_host_rerror_rate  attack_type  \n",
      "0                 -0.549814             -0.487765       normal  \n",
      "1                  1.899674             -0.487765      neptune  \n",
      "2                 -0.525319             -0.487765       normal  \n",
      "3                 -0.549814             -0.487765       normal  \n",
      "4                 -0.549814              2.476323      neptune  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "scaledFeatures.shape:  (37041, 21)\n",
      "\n",
      "****** End of feature scaling of the features present in the dataset using StandardScalar *****\n",
      "\n",
      "features.shape:  (37041, 20)\n",
      "label.shape:  (37041,)\n",
      "features:         Protocol_type   Service      Flag  Src_bytes  Logged_in     Count  \\\n",
      "0           2.038571  0.629977  0.735369  -0.011190  -0.732914 -0.581217   \n",
      "1          -0.154786  0.938904 -0.660992  -0.011262  -0.732914  0.275339   \n",
      "2          -0.154786 -0.543948  0.735369  -0.011147   1.364417 -0.643512   \n",
      "3          -0.154786 -0.543948  0.735369  -0.011163   1.364417 -0.448840   \n",
      "4          -0.154786  0.938904 -2.057353  -0.011262  -0.732914  0.259766   \n",
      "...              ...       ...       ...        ...        ...       ...   \n",
      "37036       2.038571 -1.223588  0.735369  -0.011241  -0.732914  0.532306   \n",
      "37037      -0.154786 -0.543948  0.735369  -0.011096   1.364417 -0.612365   \n",
      "37038      -0.154786  1.618544 -1.708262  -0.011262  -0.732914 -0.674660   \n",
      "37039      -0.154786  1.433188 -2.057353  -0.011262  -0.732914 -0.643512   \n",
      "37040       2.038571  0.938904  0.735369  -0.011242  -0.732914 -0.651299   \n",
      "\n",
      "       Srv_count  Serror_rate  Srv_serror_rate  Same_srv_rate  Diff_srv_rate  \\\n",
      "0      -0.367299    -0.556584        -0.552030      -1.421427       0.233303   \n",
      "1      -0.312117     1.851192         1.851769      -1.491319      -0.092358   \n",
      "2      -0.323153    -0.075029        -0.071270       0.721924      -0.377312   \n",
      "3      -0.025167    -0.556584        -0.552030       0.721924      -0.377312   \n",
      "4      -0.168642    -0.556584        -0.552030      -1.235049      -0.133066   \n",
      "...          ...          ...              ...            ...            ...   \n",
      "37036   2.226285    -0.556584        -0.552030       0.721924      -0.377312   \n",
      "37037  -0.267971    -0.556584        -0.552030       0.721924      -0.377312   \n",
      "37038  -0.301080    -0.556584        -0.215498       0.721924      -0.377312   \n",
      "37039  -0.323153    -0.556584        -0.552030      -1.141860       2.879303   \n",
      "37040  -0.334190    -0.556584        -0.552030       0.721924      -0.377312   \n",
      "\n",
      "       Dst_host_count  Dst_host_srv_count  Dst_host_same_srv_rate  \\\n",
      "0            0.656445           -1.050270               -1.193023   \n",
      "1            0.656445           -0.821669               -0.966271   \n",
      "2           -1.709884            1.272317                1.074493   \n",
      "3            0.656445            1.272317                1.074493   \n",
      "4            0.656445           -0.885678               -1.034297   \n",
      "...               ...                 ...                     ...   \n",
      "37036        0.656445            1.272317                1.074493   \n",
      "37037        0.656445            1.080292                0.893092   \n",
      "37038       -0.163882           -0.181586               -0.354042   \n",
      "37039       -0.226984           -0.583924               -0.762195   \n",
      "37040        0.656445            1.272317                1.074493   \n",
      "\n",
      "       Dst_host_diff_srv_rate  Dst_host_same_src_port_rate  \\\n",
      "0                    2.187298                     2.137976   \n",
      "1                   -0.237144                    -0.498320   \n",
      "2                   -0.457548                    -0.408446   \n",
      "3                   -0.457548                    -0.498320   \n",
      "4                   -0.148983                    -0.498320   \n",
      "...                       ...                          ...   \n",
      "37036               -0.457548                    -0.468362   \n",
      "37037               -0.369387                    -0.498320   \n",
      "37038               -0.325306                    -0.468362   \n",
      "37039               -0.325306                    -0.468362   \n",
      "37040               -0.457548                    -0.498320   \n",
      "\n",
      "       Dst_host_srv_diff_host_rate  Dst_host_serror_rate  \\\n",
      "0                        -0.263701             -0.561390   \n",
      "1                        -0.263701              1.904034   \n",
      "2                         0.094049             -0.487427   \n",
      "3                        -0.263701             -0.561390   \n",
      "4                        -0.263701             -0.561390   \n",
      "...                            ...                   ...   \n",
      "37036                    -0.263701             -0.561390   \n",
      "37037                    -0.263701             -0.561390   \n",
      "37038                    -0.084826             -0.438119   \n",
      "37039                     0.094049             -0.561390   \n",
      "37040                    -0.263701             -0.561390   \n",
      "\n",
      "       Dst_host_srv_serror_rate  Dst_host_rerror_rate  \n",
      "0                     -0.549814             -0.487765  \n",
      "1                      1.899674             -0.487765  \n",
      "2                     -0.525319             -0.487765  \n",
      "3                     -0.549814             -0.487765  \n",
      "4                     -0.549814              2.476323  \n",
      "...                         ...                   ...  \n",
      "37036                 -0.549814             -0.487765  \n",
      "37037                 -0.549814             -0.339561  \n",
      "37038                 -0.353855              2.031710  \n",
      "37039                 -0.549814              2.120633  \n",
      "37040                 -0.549814             -0.487765  \n",
      "\n",
      "[37041 rows x 20 columns]\n",
      "label:  0           normal\n",
      "1          neptune\n",
      "2           normal\n",
      "3           normal\n",
      "4          neptune\n",
      "           ...    \n",
      "37036       normal\n",
      "37037       normal\n",
      "37038        mscan\n",
      "37039        mscan\n",
      "37040    snmpguess\n",
      "Name: attack_type, Length: 37041, dtype: object\n",
      "   Protocol_type   Service      Flag  Src_bytes  Logged_in     Count  \\\n",
      "0       2.038571  0.629977  0.735369  -0.011190  -0.732914 -0.581217   \n",
      "1      -0.154786  0.938904 -0.660992  -0.011262  -0.732914  0.275339   \n",
      "2      -0.154786 -0.543948  0.735369  -0.011147   1.364417 -0.643512   \n",
      "3      -0.154786 -0.543948  0.735369  -0.011163   1.364417 -0.448840   \n",
      "4      -0.154786  0.938904 -2.057353  -0.011262  -0.732914  0.259766   \n",
      "\n",
      "   Srv_count  Serror_rate  Srv_serror_rate  Same_srv_rate  ...  \\\n",
      "0  -0.367299    -0.556584        -0.552030      -1.421427  ...   \n",
      "1  -0.312117     1.851192         1.851769      -1.491319  ...   \n",
      "2  -0.323153    -0.075029        -0.071270       0.721924  ...   \n",
      "3  -0.025167    -0.556584        -0.552030       0.721924  ...   \n",
      "4  -0.168642    -0.556584        -0.552030      -1.235049  ...   \n",
      "\n",
      "   Dst_host_count  Dst_host_srv_count  Dst_host_same_srv_rate  \\\n",
      "0        0.656445           -1.050270               -1.193023   \n",
      "1        0.656445           -0.821669               -0.966271   \n",
      "2       -1.709884            1.272317                1.074493   \n",
      "3        0.656445            1.272317                1.074493   \n",
      "4        0.656445           -0.885678               -1.034297   \n",
      "\n",
      "   Dst_host_diff_srv_rate  Dst_host_same_src_port_rate  \\\n",
      "0                2.187298                     2.137976   \n",
      "1               -0.237144                    -0.498320   \n",
      "2               -0.457548                    -0.408446   \n",
      "3               -0.457548                    -0.498320   \n",
      "4               -0.148983                    -0.498320   \n",
      "\n",
      "   Dst_host_srv_diff_host_rate  Dst_host_serror_rate  \\\n",
      "0                    -0.263701             -0.561390   \n",
      "1                    -0.263701              1.904034   \n",
      "2                     0.094049             -0.487427   \n",
      "3                    -0.263701             -0.561390   \n",
      "4                    -0.263701             -0.561390   \n",
      "\n",
      "   Dst_host_srv_serror_rate  Dst_host_rerror_rate  attack_type  \n",
      "0                 -0.549814             -0.487765       normal  \n",
      "1                  1.899674             -0.487765      neptune  \n",
      "2                 -0.525319             -0.487765       normal  \n",
      "3                 -0.549814             -0.487765       normal  \n",
      "4                 -0.549814              2.476323      neptune  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "completeEncodedAndScaledDataset = performPreprocessing(trainingDataSet, testingDataSet, arrayOfModels)\n",
    "print(completeEncodedAndScaledDataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f211df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37041, 20) (37041,)\n",
      "Number of unique values in label:  40\n",
      "Unique values in label:  ['apache2' 'back' 'buffer_overflow' 'ftp_write' 'guess_passwd'\n",
      " 'httptunnel' 'imap' 'ipsweep' 'land' 'loadmodule' 'mailbomb' 'mscan'\n",
      " 'multihop' 'named' 'neptune' 'nmap' 'normal' 'perl' 'phf' 'pod'\n",
      " 'portsweep' 'processtable' 'ps' 'rootkit' 'saint' 'satan' 'sendmail'\n",
      " 'smurf' 'snmpgetattack' 'snmpguess' 'spy' 'sqlattack' 'teardrop'\n",
      " 'udpstorm' 'warezclient' 'warezmaster' 'worm' 'xlock' 'xsnoop' 'xterm']\n",
      "(37041, 20) (37041, 40)\n"
     ]
    }
   ],
   "source": [
    "x = completeEncodedAndScaledDataset.drop('attack_type',axis=1)\n",
    "y = completeEncodedAndScaledDataset['attack_type']\n",
    "print(x.shape, y.shape)\n",
    "print('Number of unique values in label: ',len(np.unique(y)))\n",
    "print('Unique values in label: ',np.unique(y))\n",
    "#print(y.value_counts())\n",
    "onehot = pd.get_dummies(y)\n",
    "y = onehot.values\n",
    "print(x.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6109314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function is used to define, compile and filt a neural network\n",
    "'''\n",
    "def nn_model(trainx, trainy, valx,valy,bt_size,epochs, layers):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(layers[0],activation='relu', input_shape=(trainx.shape[1],)))\n",
    "  for l in layers[1:]:\n",
    "    model.add(Dense(l, activation='relu' ))\n",
    "    model.add(Dropout(0.30))\n",
    "  model.add(Dense(trainy.shape[1], activation='softmax'))\n",
    "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  hist=model.fit(trainx, trainy, batch_size=bt_size, epochs=epochs, shuffle=True, validation_data=(valx,valy), verbose=True)\n",
    "  model.save('modelnew.h5')\n",
    "  return hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cacf3d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['Protocol_type' 'Service' 'Flag' 'Src_bytes' 'Logged_in' 'Count'\n",
      " 'Srv_count' 'Serror_rate' 'Srv_serror_rate' 'Same_srv_rate'\n",
      " 'Diff_srv_rate' 'Dst_host_count' 'Dst_host_srv_count'\n",
      " 'Dst_host_same_srv_rate' 'Dst_host_diff_srv_rate'\n",
      " 'Dst_host_same_src_port_rate' 'Dst_host_srv_diff_host_rate'\n",
      " 'Dst_host_serror_rate' 'Dst_host_srv_serror_rate' 'Dst_host_rerror_rate']\n",
      "[-0.15478617  1.61854439  0.73536923 -0.0112619  -0.73291357 -0.6746598\n",
      " -0.2458976  -0.55658419 -0.35972625  0.72192361 -0.3773118  -0.1323311\n",
      " -0.8948217  -1.17034772 -0.36938667 -0.46836203  0.72011226 -0.56138989\n",
      " -0.54981386 -0.48776502]\n",
      "Epoch 1/2\n",
      "1737/1737 [==============================] - 27s 14ms/step - loss: 1.2486 - accuracy: 0.6875 - val_loss: 0.8323 - val_accuracy: 0.7587\n",
      "Epoch 2/2\n",
      "1737/1737 [==============================] - 26s 15ms/step - loss: 0.8740 - accuracy: 0.7479 - val_loss: 0.6428 - val_accuracy: 0.8056\n"
     ]
    }
   ],
   "source": [
    "trainx, testx, trainy, testy = train_test_split(x,y, test_size=0.25, random_state=42)\n",
    "layers=[trainx.shape[1],800,500,400,300,200,100,50,10]\n",
    "print(trainx.shape[1])\n",
    "print(trainx.columns.values)\n",
    "print(trainx.iloc[0].values)\n",
    "hist = nn_model(trainx, trainy, testx, testy,16,2,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "666168fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(trainy.shape[1])\n",
    "print(trainy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0795cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ab5c4",
   "metadata": {},
   "source": [
    "# \"\"\"\n",
    "['Protocol_type' 'Service' 'Flag' 'Src_bytes' 'Logged_in' 'Count'\n",
    " 'Srv_count' 'Serror_rate' 'Srv_serror_rate' 'Rerror_rate' 'Same_srv_rate'\n",
    " 'Diff_srv_rate' 'Dst_host_count' 'Dst_host_srv_count'\n",
    " 'Dst_host_same_srv_rate' 'Dst_host_diff_srv_rate'\n",
    " 'Dst_host_same_src_port_rate' 'Dst_host_serror_rate'\n",
    " 'Dst_host_srv_serror_rate' 'Dst_host_rerror_rate']\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "n label:  40\n",
    "Unique values in label:  [\n",
    "    0         1            2             3           4\n",
    " 'apache2' 'back' 'buffer_overflow' 'ftp_write' 'guess_passwd'\n",
    "    5           6        7        8        9\n",
    " 'httptunnel' 'imap' 'ipsweep' 'land' 'loadmodule' \n",
    "  10          11        12        13     14\n",
    " 'mailbomb' 'mscan' 'multihop' 'named' 'neptune' \n",
    " 15        16      17      18   19\n",
    " 'nmap' 'normal' 'perl' 'phf' 'pod'\n",
    " 20             21           22    23         24\n",
    " 'portsweep' 'processtable' 'ps' 'rootkit' 'saint' \n",
    "  25        26          27        28          29\n",
    " 'satan' 'sendmail' 'smurf' 'snmpgetattack' 'snmpguess' \n",
    " 30         31       32         33          34\n",
    " 'spy' 'sqlattack' 'teardrop' 'udpstorm' 'warezclient' \n",
    " 35               36      37      38     39\n",
    " 'warezmaster' 'worm' 'xlock' 'xsnoop' 'xterm']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f599856",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=['apache2','back', 'buffer_overflow', 'ftp_write', 'guess_passwd',\n",
    " 'httptunnel', 'imap', 'ipsweep', 'land', 'loadmodule', 'mailbomb', 'mscan',\n",
    " 'multihop', 'named', 'neptune', 'nmap', 'normal', 'perl', 'phf', 'pod',\n",
    " 'portsweep', 'processtable', 'ps', 'rootkit', 'saint', 'satan', 'sendmail',\n",
    " 'smurf', 'snmpgetattack', 'snmpguess', 'spy', 'sqlattack', 'teardrop',\n",
    " 'udpstorm', 'warezclient', 'warezmaster', 'worm', 'xlock', 'xsnoop', 'xterm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37234180",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25076\\525703143.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =tensorflow.keras.models.load_model('modelnew.h5')\n",
    "test_data = np.array([[-0.15478617,1.61854439,  0.73536923, -0.0112619 , -0.73291357, -0.6746598,-0.2458976 , -0.55658419, -0.35972625 ,-0.4608058,   0.72192361 ,-0.3773118,-0.1323311 , -0.8948217,  -1.17034772, -0.36938667, -0.46836203 ,0.72011226, -0.56138989, -0.54981386 ,-0.48776502]])\n",
    "o=model.predict(test_data, batch_size=1)\n",
    "print(o)\n",
    "print(len(o[0]))\n",
    "print(o.argmax())\n",
    "print(result[int(o.argmax())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f1e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc532b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34469d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9fc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "features = dataSetInArrayFormat[:,0:numberOfColumnsInEncodedDataset-1]\n",
    "\n",
    "#Perform feature scaling\n",
    "scaler=StandardScaler()\n",
    "df1=scaler.fit_transform(np.array(df1))\n",
    "df1=scaler.fit_transform(np.array(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f97cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a017ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b4051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecbf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
